pip install gdown h5py
gdown --id '18HjAaZb26XCufJm5H-fD99-Z0j2u_0LS' -O upch-ml.zip
unzip upch-ml.zip
# -*- coding: utf-8 -*-
"""Copia de Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nNBDMBjeOQETStDDtkgDZGzilfiUeYW3
"""

import pandas as pd
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
import os

# Leer el DataFrame de entrenamiento
train_df = pd.read_csv('train.csv')

# Crear nombres de archivos
train_df['filename'] = train_df['ID'] + '_' + train_df['location'] + '.jpg'
train_df['level'] = train_df['level'].apply(lambda x: '0' if x == 0 else '1')

# Definir par치metros de imagen y entrenamiento
img_height, img_width = 224, 224
batch_size = 32
num_classes = 2

# Dividir en conjunto de entrenamiento y validaci칩n
train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['level'], random_state=42)

# Generadores de datos
train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
val_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    directory='train/train',
    x_col='filename',
    y_col='level',
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='sparse'
)

validation_generator = val_datagen.flow_from_dataframe(
    dataframe=val_df,
    directory='train/train',
    x_col='filename',
    y_col='level',
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode='sparse'
)

# Construir el modelo
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

from tensorflow.keras.metrics import AUC
model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy', AUC()])

model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Entrenar el modelo
model.fit(
    train_generator,
    steps_per_epoch=train_generator.n // batch_size,
    epochs=11,
    validation_data=validation_generator,
    validation_steps=validation_generator.n // batch_size
)

# Guardar el modelo
model.save('retinopathy_detection_model.h5')

# Generador para el conjunto de prueba
test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    '/content/test',
    target_size=(img_height, img_width),
    batch_size=batch_size,
    class_mode=None,
    shuffle=False
)

# Verificar archivos en el directorio de prueba
test_dir = '/content/test/test'
if not os.path.exists(test_dir):
    print(f"El directorio {test_dir} no existe.")
else:
    test_files = os.listdir(test_dir)
    if len(test_files) == 0:
        print(f"No se encontraron im치genes en el directorio {test_dir}.")
    else:
        print(f"Se encontraron {len(test_files)} im치genes en el directorio {test_dir}.")

        # Predecir resultados en el conjunto de prueba
        predictions = model.predict(test_generator, steps=len(test_generator), verbose=1)

        # Extraer puntajes flotantes
        scores = predictions[:, 1]

        # Crear DataFrame de resultados
        submission_df = pd.DataFrame({
            'ID': [os.path.splitext(os.path.basename(f))[0] for f in test_generator.filenames],
            'score': scores
        })
        submission_df.to_csv('submissionDR.csv', index=False)

        print(submission_df.head())
